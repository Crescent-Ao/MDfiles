### 线性代数

https://colab.research.google.com/drive/1Od4TVJ1zje20lJO3uMvwdAC9h-1vza1g#scrollTo=_ESwKxA_Mafw

![image-20210603125628053](C:\Users\王澳\AppData\Roaming\Typora\typora-user-images\image-20210603125628053.png)

矩阵向量的乘积

矩阵的乘积

```python
torch.mm
torch.mv矩阵和向量的乘法
```

L2矩阵范数是向量元素的平方和的平方根

```python
u=torch.tensor([3.,-4])
torch.norm(u)得到的结果将是变量
```

L1范数向量元素的绝对值之和

```python
torch.abs(u).sum()
```

![image-20210603130014449](C:\Users\王澳\AppData\Roaming\Typora\typora-user-images\image-20210603130014449.png)

![image-20210603130644346](C:\Users\王澳\AppData\Roaming\Typora\typora-user-images\image-20210603130644346.png)

#### 矩阵运算和怎么求导数

![image-20210603131917526](C:\Users\王澳\AppData\Roaming\Typora\typora-user-images\image-20210603131917526.png)

![image-20210603132010767](C:\Users\王澳\AppData\Roaming\Typora\typora-user-images\image-20210603132010767.png)

向量关于向量的矩阵

![image-20210603132057390](C:\Users\王澳\AppData\Roaming\Typora\typora-user-images\image-20210603132057390.png)

![image-20210603132441877](C:\Users\王澳\AppData\Roaming\Typora\typora-user-images\image-20210603132441877.png)

![image-20210603132657338](C:\Users\王澳\AppData\Roaming\Typora\typora-user-images\image-20210603132657338.png)

![image-20210603132913723](C:\Users\王澳\AppData\Roaming\Typora\typora-user-images\image-20210603132913723.png)

![image-20210603133349503](C:\Users\王澳\AppData\Roaming\Typora\typora-user-images\image-20210603133349503.png)

计算图是无环的计算图

![image-20210603133727729](C:\Users\王澳\AppData\Roaming\Typora\typora-user-images\image-20210603133727729.png)

前向计算，执行图，存储中中间的结果。反向：从相反方向执行计算图，去除不需要的值，两个导数值不需要的值就不计算了

计算复杂度,正向和反向的代价是一样的，但是内存复杂度是不一样的

![image-20210603134839255](C:\Users\王澳\AppData\Roaming\Typora\typora-user-images\image-20210603134839255.png)

![image-20210608110439576](C:\Users\王澳\AppData\Roaming\Typora\typora-user-images\image-20210608110439576.png)

计算图两种链式法则的计算法则，反向传递的方法

![image-20210608113119942](C:\Users\王澳\AppData\Roaming\Typora\typora-user-images\image-20210608113119942.png)

![image-20210608113506198](C:\Users\王澳\AppData\Roaming\Typora\typora-user-images\image-20210608113506198.png)

![image-20210608113708818](C:\Users\王澳\AppData\Roaming\Typora\typora-user-images\image-20210608113708818.png)

![image-20210608113943912](C:\Users\王澳\AppData\Roaming\Typora\typora-user-images\image-20210608113943912.png)

线性回归可以看成一个单层神经网络，线性回归有显示解，n维输入的加权外加偏差。

### 基础优化的方法

梯度下降

- 挑选参数的随机初始值$w_0$

  ![image-20210608114233730](C:\Users\王澳\AppData\Roaming\Typora\typora-user-images\image-20210608114233730.png)

梯度下降的方法，其实很简单，前向传播，反向传播，梯度是指函数下降速度最快的方向，学习率步长的超参数，梯度下降的直观解释。小批量随机梯度下降，在整个训练集上计算梯度的代价太大，可以采样b个样本用来近似损失，b是批量大小。

![image-20210608153721902](C:\Users\王澳\AppData\Roaming\Typora\typora-user-images\image-20210608153721902.png)

梯度下降沿着梯度的反方向更新参数求解，小批量的随机梯度下降是深度学习默认的求解算法，两个重要的超参数时批量的大小和学习

```python
def synthetic_data(w,b,num_example):
  x=torch.normal(0,1,(num_example,len(w)))#均值为0 方差为1 的随机数
  y=torch.matmul(x,w)+b
  y+=torch.normal(0,0.01,)
return x,y.reshape((-1,1))
true_w=torch.tensor([2,-3.4])
true_4.2
feature,labels=syntetic_data(true_w,true_b,1000)#用实际的结果生成验证和标注
```

![image-20210608154918472](C:\Users\王澳\AppData\Roaming\Typora\typora-user-images\image-20210608154918472.png)

```python
def data_iter(batch_size,features,labels):
    num_examples=len(features)
    indices=list(range(num_examples))
    random.shuffle(indices)
    for i in range(0,num_examples,batch_size):
        batch_indices=torch.tensor(indices[i:min(i+batch_size,num_examples)])
        yield features[batch_indices],labels[batch_indices]

    
```

![image-20210608155407249](C:\Users\王澳\AppData\Roaming\Typora\typora-user-images\image-20210608155407249.png)

```python
w=torch.normal(0,0.01,size=(2,1),requires_grad=True)
def linreg(x,w,b):
    return torch.matmul(x,w)+b
def squared_loss(y_hat,y):
    return(y_hat-y.reshape(y_hat.shape))**2/2
def sgd(params,lr,batch_size):
    with torch.no_grad():
        for param in params:
            param-=lr*param.grad/batch_size
            param.grad_zero_()
```

```python
lr=0.03
num_epochs=3
net=linreg
loss=squared_loss
for epoch in range(num_epochs):
    for x,y in data_iter(batch_size,featurs,lables):
        l=loss(net(x,w,b),y)
        l.sum().backward()
        sgd([w,b],e,batch_size)
  
```

![image-20210608160302472](C:\Users\王澳\AppData\Roaming\Typora\typora-user-images\image-20210608160302472.png)

回归与分类的问题

- 回归估计一个连续值
- 分类预测一个离散的类别
- MNIST或者ImageNet自然物体分类

从回归到多类分类-均方损失

- 对类别进行一位有效编码

- 使用均方损失进行训练

- 最大值用来预测

- 正确类别的正确度特别大

  ![image-20210610170824288](C:\Users\王澳\AppData\Roaming\Typora\typora-user-images\image-20210610170824288.png)

  

![image-20210610170948523](C:\Users\王澳\AppData\Roaming\Typora\typora-user-images\image-20210610170948523.png)

输出的是概率，只有一个元素为1，其他的元素为0，通过两个概率的差值计算对应的算是函数

![image-20210610171057645](C:\Users\王澳\AppData\Roaming\Typora\typora-user-images\image-20210610171057645.png)

![image-20210610171235986](C:\Users\王澳\AppData\Roaming\Typora\typora-user-images\image-20210610171235986.png)

损失函数用来衡量真实值和缺失值之间的区别
$$
L2:l（y,y^‘)=\frac{1}{2}(y-y')^2
$$

$$
l(y,y')=|y-y'|
$$

![image-20210610171939550](C:\Users\王澳\AppData\Roaming\Typora\typora-user-images\image-20210610171939550.png)

不管里的多远，回去都是同样的芫荽

![image-20210610172118672](C:\Users\王澳\AppData\Roaming\Typora\typora-user-images\image-20210610172118672.png)

```python
%matplotlib inline
import torch
import torchvision
from torch.utils import data
from torchvision import transforms
from d2l import torch as d2l
d2l.use_svg_display()#清晰度较高
trans=transform.ToTensor()#做一个最简单的预处理
minist=torchvision.datasets.FashionMNIST(root="../data",train=true,transform=trans,download=True)
minist=torchvision.datasets.FashionMNIST(root="../data",train=Fale,transform=trans,download=True)
minist_train[0][0].shape
```

```python
def get_fashion_mini
```

![image-20210610185037375](C:\Users\王澳\AppData\Roaming\Typora\typora-user-images\image-20210610185037375.png)

```python
x,y=next(iter(data.Dataloader(minist_train,batch_size=18)))
show_images(X.reshape(18,28,28),2,9,titles_get_fashion_minist_labels)#丢弃掉原来的channels，将图片拿出来
```

```python
def get_dataloader_workers():
    return 4//四个进程
train_iter=data.DataLoader(minist_train,batch_size,shuffle=True,num_workers=get_loader_worker)//来加速数据读取
timer=d2l.Timer
for X,y in train_iter:
    continue
print(f'{timer.stop:.2f}sec')
```

### softmax从零开始实现

```python
import torch
batch_size=256
train_iter,test_iter=d2l.load_data_fashion_mnist(batch_size)
num_inputs=784
num_outputs=10
w=torch.normal(0,0.01,size=(num_inputs,num_outputs),requires_grad=True)#向量
b=torch.zeros(num_outputs,requires_grad=True)#向量
def softmax(x):
    X_exp=torch.exp(x)
    partition=X_exp(1,keepdim=True)#每一行进行来求和
    return X_exp/partition#运用了
```

![image-20210610191208618](C:\Users\王澳\AppData\Roaming\Typora\typora-user-images\image-20210610191208618.png)

```python 
def net(x):
    return softmax(torch.matmul(x.reshape(-1,W,shape[0])),W)+b)
```

![image-20210610191813267](C:\Users\王澳\AppData\Roaming\Typora\typora-user-images\image-20210610191813267.png)

如果是由三类，第0样本y[0]，第一个样本y[1]的元素，给了

```python
def cross_enyropy(y_hat,y):
    return -torch.log(y_hat[range(len(y_hat)),y])
cross_entropy(y_hat,y)
def acurracy(y_hat,y):
    if(len(y_hat.shape)>1 and y_hat.shape[1]>1):
        y_hat=np.argmax(axis=1)
    cmp=y_hat.type(y.dtype)==y
    return float(cmp.type(y.dtype).sum())
def evaluate_acuuracy(net,data_iter):
    if isinstance(net,torch.nn.Module):
        net.eval()#评估模式
    metric=Accunulator(2)
    for x,y in data_iter:
        metric.add(acuuracy(net(X),y),y.numel())
    return metric[0]/metric[1]
```



![image-20210610193548161](C:\Users\王澳\AppData\Roaming\Typora\typora-user-images\image-20210610193548161.png)

```python
def train_epoch_ch3(net,train_iter,loss,updater):
    if(isinstance(net,torch.nn.Module)):
        net.train()
    metric=Accumlator(3)
    for x,y in train_iter:
        y_hat=net(x)
        l=loss(y_hat,y)
        if(ininstance(updater,torch.optim.Optimizer)):
            updater.zero_grad()
            l.backward()
            updater.step()
            metric.add()
```

![image-20210610193911735](C:\Users\王澳\AppData\Roaming\Typora\typora-user-images\image-20210610193911735.png)

### softmax简洁实现

```python
net=nn.sequential(nn.Flatten(),nn.Linear(784,10))
def init_weights(m):
    if(type(m)==nn.Linear):
        nn.init.normal_(m.weight,std=0.01)
net.apply(init_weights)
loss=nn.CrossEbtropyLoss()
trainer=torch.optim.SGD(net.parameters,lr=0.01)
num_epochs=10

```

![image-20210610194745971](C:\Users\王澳\AppData\Roaming\Typora\typora-user-images\image-20210610194745971.png)

### 60年代感知机

![image-20210610201126634](C:\Users\王澳\AppData\Roaming\Typora\typora-user-images\image-20210610201126634.png)

![image-20210610201629707](C:\Users\王澳\AppData\Roaming\Typora\typora-user-images\image-20210610201629707.png)
$$
y(x^Tw+b)>\rho
$$
![image-20210610202149491](C:\Users\王澳\AppData\Roaming\Typora\typora-user-images\image-20210610202149491.png)

![image-20210610202401475](C:\Users\王澳\AppData\Roaming\Typora\typora-user-images\image-20210610202401475.png)

- 感知机是一个二分类的问题，最早的AI模型
- 等价于批量大小为1的梯度下降
- 不能拟合XOR函数

### 多层感知机

#### 如何学习XOR

存在四个点，怎么完全分类的方法，几步来完成对应的方法，

蓝色的线是根据x的进行分类，3和4是进行黄色的分类，然后进行一个异或的运算，这样就可以进行一个分类，

![image-20210610203048126](C:\Users\王澳\AppData\Roaming\Typora\typora-user-images\image-20210610203048126.png)

组合的方法，从一层编程了多层，简单来说

![image-20210610203744073](C:\Users\王澳\AppData\Roaming\Typora\typora-user-images\image-20210610203744073.png)

输入的大小是不能改变，输出的大小是有多少类

![image-20210610204040321](C:\Users\王澳\AppData\Roaming\Typora\typora-user-images\image-20210610204040321.png)

输出层单位层，激活函数的作用，非线性函数，仍然是线性函数，实际上还是最简单的函数

计算机视觉的神经网络结构

- 平移不变性：不管出现在图像中的哪个位置，神经网络的底层应该对相同的图像区域做出类似的响应。这个原理即为“平移不变性”。
- 局部性：神经网络的底层应该只探索输入图像中的局部区域，而不考虑图像远处区域的内容，这就是“局部性”原则。最终，这些局部特征可以融会贯通，在整个图像级别上做出预测。

#### VGG

组成卷积块，然后将卷积层，提出了一种VGG块的理念

- 3$\times$3,padding=1(n层m通道)同样计算的情况下，模型更深比窗口更小的效果更好
- 2$\times$2最大池化层层
- 不同次数的重复块，可以得到不同的架构VGG-16,VGG-19,16个卷积层，19个卷积层，4096,4084,10的Dense层，n块东西串在一起
- 更大更深的AlexNet(重复的VGG块)

##### summary

- 使用可重复使用的块构建深度卷积神经网络
- 不同的卷积块和超参数的个数可以得到不同复杂度的变种

##### Code

```python 
import torch 
from torch import nn
from d2l import torch as d2l
def vgg_block(in_channels,out_channels,num_convs):
    layers=[]
    for _ in range(num_convs):
        layers.append(nn.Conv2d(in_channels,out_chaneels,kernel_size=(3,3),padding=1))
        layer.append(nn.ReLU())
        in_channels=out_channels
   	layers.append(nn.MaxPool2d(kernel_size=2,stride=2))
   	return nn.Sequential(*layers)
conv_arch=((1,64),(1,128),(2,256),(2,512),(2,512))#224/5=7每次除2，最后的池化的作用
def vgg(conv_arch):
    conv_blks=[]
    in_channels=1
    for (in_channels,out_channels) in conv_arch:
        conv_blks,append(vgg_block(num_convs,in_channels,out_channels))
    return nn.Sequential(*conv_blks,nn.Flatten(),nn.Linear(out_channels*7*7,4096),nn.ReLU(),nn.DropOut(0.5),nn.Linear(4096,4096),nn.ReLU(),nn.DropOut(0.5),nn.Linear(4096,10))
```

更小的窗口，更深的层次，来解决对应的问题

#### NiN网络中的网络

##### 全连接层的问题

- 卷积层需要较小的参数，网络参数$c_i\times c_0\times k^2$
- 卷积层后的第一个全连接层的参数，全连接层占用大量的内存，占用大量的计算带宽
  - LeNet 16*5*5*120
  - AlexNet 256*5*5*4096
  - 512\*7\*7\*4096VGG

##### NiN块的解决办法

- 一个卷积层跟两个全连接层

  - 1\*1的卷积层等价于全连接层的作用，输出形状跟卷积层的输出一样
  - 将通道数进行混合

  ![image-20210627095044278](C:\Users\王澳\AppData\Roaming\Typora\typora-user-images\image-20210627095044278.png)

  - 起到全连接层的作用

##### 整体架构

- 无全连接层
- 交替使用NiN块和stride=2的最大池化层,kernel_size=2
- 减少高宽和增大通道数
- 最后使用全局平均池化层得到输出
- 其输入通道数时类别数

##### summary

- NiN块使用卷积层加两个1\*1的卷积层
- - 后者对每个像素增加了非线性
- 不容易过拟合，更少的参数个数

````python
import torch
from torch import nn
def nin_block(in_channels,out_channels,strides,padding,kernel_size):
    return nn.Sequential(nn.Conv2d(in_channels,out_channels,kernel_size,nn.Conv2d(out_channels,out_channels.kernel_size=1),nn.ReLU(),nn.Conv2d(out_channels,out_channels,kernel_size=1.nn.ReLU())))
````

![image-20210627100134192](C:\Users\王澳\AppData\Roaming\Typora\typora-user-images\image-20210627100134192.png)

![image-20210627100504016](C:\Users\王澳\AppData\Roaming\Typora\typora-user-images\image-20210627100504016.png)

##### 整体架构同AlexNet格式大致相同，最后全局平均池化层的作用

#### pytorch基础

```python
import torch  
from torch import nn
from torch.nn import functional as F
net=nn.Sequential(nn.Linear(20,256),nn.ReLU(),)
```

